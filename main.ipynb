{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:07:16.468614Z",
     "iopub.status.busy": "2021-01-12T10:07:16.468281Z",
     "iopub.status.idle": "2021-01-12T10:07:16.556090Z",
     "shell.execute_reply": "2021-01-12T10:07:16.554734Z",
     "shell.execute_reply.started": "2021-01-12T10:07:16.468573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import Sequence, StripAccents, Lowercase, Strip\n",
    "from tokenizers.pre_tokenizers import Sequence as PreSequence\n",
    "from tokenizers.pre_tokenizers import Whitespace, Punctuation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import dataset\n",
    "import model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T09:40:35.832417Z",
     "iopub.status.busy": "2021-01-12T09:40:35.832096Z",
     "iopub.status.idle": "2021-01-12T09:40:35.911403Z",
     "shell.execute_reply": "2021-01-12T09:40:35.909708Z",
     "shell.execute_reply.started": "2021-01-12T09:40:35.832376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_PROJECT=squad-qa\n",
      "env: WANDB_DISABLED=false\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.xmargin'] = .05\n",
    "plt.rcParams['axes.ymargin'] = .05\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_PROJECT=squad-qa\n",
    "%env WANDB_DISABLED=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T15:11:25.843689Z",
     "iopub.status.busy": "2021-01-09T15:11:25.843217Z",
     "iopub.status.idle": "2021-01-09T15:11:28.032758Z",
     "shell.execute_reply": "2021-01-09T15:11:28.031025Z",
     "shell.execute_reply.started": "2021-01-09T15:11:25.843635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwadaboa\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading/preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:37:29.353784Z",
     "iopub.status.busy": "2021-01-12T10:37:29.353338Z",
     "iopub.status.idle": "2021-01-12T10:37:29.749821Z",
     "shell.execute_reply": "2021-01-12T10:37:29.748398Z",
     "shell.execute_reply.started": "2021-01-12T10:37:29.353739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>188</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>279</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>381</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>92</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>229</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>414</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>476</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>1975</td>\n",
       "      <td>199</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      5733be284776f41900661182   \n",
       "1      5733be284776f4190066117f   \n",
       "2      5733be284776f41900661180   \n",
       "3      5733be284776f41900661181   \n",
       "4      5733be284776f4190066117e   \n",
       "...                         ...   \n",
       "87594  5735d259012e2f140011a09d   \n",
       "87595  5735d259012e2f140011a09e   \n",
       "87596  5735d259012e2f140011a09f   \n",
       "87597  5735d259012e2f140011a0a0   \n",
       "87598  5735d259012e2f140011a0a1   \n",
       "\n",
       "                                                question  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595               What was Yangon previously known as?   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598                      What is KMC an initialism of?   \n",
       "\n",
       "                          title  context_id  \\\n",
       "0      University_of_Notre_Dame           0   \n",
       "1      University_of_Notre_Dame           0   \n",
       "2      University_of_Notre_Dame           0   \n",
       "3      University_of_Notre_Dame           0   \n",
       "4      University_of_Notre_Dame           0   \n",
       "...                         ...         ...   \n",
       "87594                 Kathmandu       18890   \n",
       "87595                 Kathmandu       18890   \n",
       "87596                 Kathmandu       18890   \n",
       "87597                 Kathmandu       18890   \n",
       "87598                 Kathmandu       18890   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87598  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                        answer  answer_start  answer_end  \n",
       "0                   Saint Bernadette Soubirous           515         541  \n",
       "1                    a copper statue of Christ           188         213  \n",
       "2                            the Main Building           279         296  \n",
       "3      a Marian place of prayer and reflection           381         420  \n",
       "4           a golden statue of the Virgin Mary            92         126  \n",
       "...                                        ...           ...         ...  \n",
       "87594                                   Oregon           229         235  \n",
       "87595                                  Rangoon           414         421  \n",
       "87596                                    Minsk           476         481  \n",
       "87597                                     1975           199         203  \n",
       "87598              Kathmandu Metropolitan City             0          27  \n",
       "\n",
       "[87599 rows x 8 columns]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset = dataset.SquadDataset()\n",
    "squad_dataset.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:37:35.231768Z",
     "iopub.status.busy": "2021-01-12T10:37:35.231321Z",
     "iopub.status.idle": "2021-01-12T10:37:35.329418Z",
     "shell.execute_reply": "2021-01-12T10:37:35.328199Z",
     "shell.execute_reply.started": "2021-01-12T10:37:35.231717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>188</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>279</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>381</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>92</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70185</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>229</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70186</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>414</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70187</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>476</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70188</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>1975</td>\n",
       "      <td>199</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70189</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70190 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      5733be284776f41900661182   \n",
       "1      5733be284776f4190066117f   \n",
       "2      5733be284776f41900661180   \n",
       "3      5733be284776f41900661181   \n",
       "4      5733be284776f4190066117e   \n",
       "...                         ...   \n",
       "70185  5735d259012e2f140011a09d   \n",
       "70186  5735d259012e2f140011a09e   \n",
       "70187  5735d259012e2f140011a09f   \n",
       "70188  5735d259012e2f140011a0a0   \n",
       "70189  5735d259012e2f140011a0a1   \n",
       "\n",
       "                                                question  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "70185  In what US state did Kathmandu first establish...   \n",
       "70186               What was Yangon previously known as?   \n",
       "70187  With what Belorussian city does Kathmandu have...   \n",
       "70188  In what year did Kathmandu create its initial ...   \n",
       "70189                      What is KMC an initialism of?   \n",
       "\n",
       "                          title  context_id  \\\n",
       "0      University_of_Notre_Dame           0   \n",
       "1      University_of_Notre_Dame           0   \n",
       "2      University_of_Notre_Dame           0   \n",
       "3      University_of_Notre_Dame           0   \n",
       "4      University_of_Notre_Dame           0   \n",
       "...                         ...         ...   \n",
       "70185                 Kathmandu       18890   \n",
       "70186                 Kathmandu       18890   \n",
       "70187                 Kathmandu       18890   \n",
       "70188                 Kathmandu       18890   \n",
       "70189                 Kathmandu       18890   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "70185  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "70186  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "70187  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "70188  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "70189  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                        answer  answer_start  answer_end  \n",
       "0                   Saint Bernadette Soubirous           515         541  \n",
       "1                    a copper statue of Christ           188         213  \n",
       "2                            the Main Building           279         296  \n",
       "3      a Marian place of prayer and reflection           381         420  \n",
       "4           a golden statue of the Virgin Mary            92         126  \n",
       "...                                        ...           ...         ...  \n",
       "70185                                   Oregon           229         235  \n",
       "70186                                  Rangoon           414         421  \n",
       "70187                                    Minsk           476         481  \n",
       "70188                                     1975           199         203  \n",
       "70189              Kathmandu Metropolitan City             0          27  \n",
       "\n",
       "[70190 rows x 8 columns]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:37:36.456179Z",
     "iopub.status.busy": "2021-01-12T10:37:36.455863Z",
     "iopub.status.idle": "2021-01-12T10:37:36.548898Z",
     "shell.execute_reply": "2021-01-12T10:37:36.547560Z",
     "shell.execute_reply.started": "2021-01-12T10:37:36.456138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>55</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>55</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>55</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>55</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>55</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17404</th>\n",
       "      <td>5732868bb3a91d1900202e0f</td>\n",
       "      <td>At what Augusta hole was the Eisenhower Pine l...</td>\n",
       "      <td>Dwight_D._Eisenhower</td>\n",
       "      <td>18592</td>\n",
       "      <td>A loblolly pine, known as the \"Eisenhower Pine...</td>\n",
       "      <td>17th</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17405</th>\n",
       "      <td>5732868bb3a91d1900202e10</td>\n",
       "      <td>How many meters away from the Masters tee on A...</td>\n",
       "      <td>Dwight_D._Eisenhower</td>\n",
       "      <td>18592</td>\n",
       "      <td>A loblolly pine, known as the \"Eisenhower Pine...</td>\n",
       "      <td>192</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17406</th>\n",
       "      <td>5732868bb3a91d1900202e11</td>\n",
       "      <td>What did Eisenhower want to be done to the Eis...</td>\n",
       "      <td>Dwight_D._Eisenhower</td>\n",
       "      <td>18592</td>\n",
       "      <td>A loblolly pine, known as the \"Eisenhower Pine...</td>\n",
       "      <td>cut down</td>\n",
       "      <td>279</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17407</th>\n",
       "      <td>5732868bb3a91d1900202e12</td>\n",
       "      <td>What damaged the Eisenhower Pine in February 2...</td>\n",
       "      <td>Dwight_D._Eisenhower</td>\n",
       "      <td>18592</td>\n",
       "      <td>A loblolly pine, known as the \"Eisenhower Pine...</td>\n",
       "      <td>ice storm</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17408</th>\n",
       "      <td>5732868bb3a91d1900202e13</td>\n",
       "      <td>In what year did Eisenhower propose that the p...</td>\n",
       "      <td>Dwight_D._Eisenhower</td>\n",
       "      <td>18592</td>\n",
       "      <td>A loblolly pine, known as the \"Eisenhower Pine...</td>\n",
       "      <td>1956</td>\n",
       "      <td>237</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17409 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      56be85543aeaaa14008c9063   \n",
       "1      56be85543aeaaa14008c9065   \n",
       "2      56be85543aeaaa14008c9066   \n",
       "3      56bf6b0f3aeaaa14008c9601   \n",
       "4      56bf6b0f3aeaaa14008c9602   \n",
       "...                         ...   \n",
       "17404  5732868bb3a91d1900202e0f   \n",
       "17405  5732868bb3a91d1900202e10   \n",
       "17406  5732868bb3a91d1900202e11   \n",
       "17407  5732868bb3a91d1900202e12   \n",
       "17408  5732868bb3a91d1900202e13   \n",
       "\n",
       "                                                question  \\\n",
       "0               When did Beyonce start becoming popular?   \n",
       "1      What areas did Beyonce compete in when she was...   \n",
       "2      When did Beyonce leave Destiny's Child and bec...   \n",
       "3          In what city and state did Beyonce  grow up?    \n",
       "4             In which decade did Beyonce become famous?   \n",
       "...                                                  ...   \n",
       "17404  At what Augusta hole was the Eisenhower Pine l...   \n",
       "17405  How many meters away from the Masters tee on A...   \n",
       "17406  What did Eisenhower want to be done to the Eis...   \n",
       "17407  What damaged the Eisenhower Pine in February 2...   \n",
       "17408  In what year did Eisenhower propose that the p...   \n",
       "\n",
       "                      title  context_id  \\\n",
       "0                   Beyoncé          55   \n",
       "1                   Beyoncé          55   \n",
       "2                   Beyoncé          55   \n",
       "3                   Beyoncé          55   \n",
       "4                   Beyoncé          55   \n",
       "...                     ...         ...   \n",
       "17404  Dwight_D._Eisenhower       18592   \n",
       "17405  Dwight_D._Eisenhower       18592   \n",
       "17406  Dwight_D._Eisenhower       18592   \n",
       "17407  Dwight_D._Eisenhower       18592   \n",
       "17408  Dwight_D._Eisenhower       18592   \n",
       "\n",
       "                                                 context               answer  \\\n",
       "0      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...    in the late 1990s   \n",
       "1      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  singing and dancing   \n",
       "2      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...                 2003   \n",
       "3      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...       Houston, Texas   \n",
       "4      Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...           late 1990s   \n",
       "...                                                  ...                  ...   \n",
       "17404  A loblolly pine, known as the \"Eisenhower Pine...                 17th   \n",
       "17405  A loblolly pine, known as the \"Eisenhower Pine...                  192   \n",
       "17406  A loblolly pine, known as the \"Eisenhower Pine...             cut down   \n",
       "17407  A loblolly pine, known as the \"Eisenhower Pine...            ice storm   \n",
       "17408  A loblolly pine, known as the \"Eisenhower Pine...                 1956   \n",
       "\n",
       "       answer_start  answer_end  \n",
       "0               269         286  \n",
       "1               207         226  \n",
       "2               526         530  \n",
       "3               166         180  \n",
       "4               276         286  \n",
       "...             ...         ...  \n",
       "17404            74          78  \n",
       "17405           110         113  \n",
       "17406           279         287  \n",
       "17407           478         487  \n",
       "17408           237         241  \n",
       "\n",
       "[17409 rows x 8 columns]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:37:37.710198Z",
     "iopub.status.busy": "2021-01-12T10:37:37.709871Z",
     "iopub.status.idle": "2021-01-12T10:37:37.788191Z",
     "shell.execute_reply": "2021-01-12T10:37:37.787034Z",
     "shell.execute_reply.started": "2021-01-12T10:37:37.710158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 515,\n",
       " 541)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:03.858823Z",
     "iopub.status.busy": "2021-01-10T15:32:03.858520Z",
     "iopub.status.idle": "2021-01-10T15:32:03.924776Z",
     "shell.execute_reply": "2021-01-10T15:32:03.923454Z",
     "shell.execute_reply.started": "2021-01-10T15:32:03.858782Z"
    }
   },
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "GLOVE_EMBEDDING_DIMENSION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:04.522025Z",
     "iopub.status.busy": "2021-01-10T15:32:04.521709Z",
     "iopub.status.idle": "2021-01-10T15:32:56.612295Z",
     "shell.execute_reply": "2021-01-10T15:32:56.610748Z",
     "shell.execute_reply.started": "2021-01-10T15:32:04.521984Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_embedding_model(model_type, embedding_dimension=50):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library\n",
    "    \"\"\"\n",
    "    # Find the correct embedding model name\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == \"word2vec\":\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "    elif model_type.strip().lower() == \"glove\":\n",
    "        download_path = f\"glove-wiki-gigaword-{embedding_dimension}\"\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"Unsupported embedding model type (choose from {word2vec, glove})\"\n",
    "        )\n",
    "\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name. Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: {300}\")\n",
    "        print(\"GloVe: {50, 100, 200, 300}\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "\n",
    "\n",
    "glove_embedding_model = load_embedding_model(\"glove\", embedding_dimension=GLOVE_EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.245672Z",
     "iopub.status.busy": "2021-01-10T15:32:57.245357Z",
     "iopub.status.idle": "2021-01-10T15:32:57.386124Z",
     "shell.execute_reply": "2021-01-10T15:32:57.384943Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.245632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12920076, -0.28866628, -0.01224866, -0.05676644, -0.20210965,\n",
       "       -0.08389011,  0.33359843,  0.16045167,  0.03867431,  0.17833012,\n",
       "        0.04696583, -0.00285802,  0.29099807,  0.04613704, -0.20923874,\n",
       "       -0.06613114, -0.06822549,  0.07665912,  0.3134014 ,  0.17848536,\n",
       "       -0.1225775 , -0.09916984, -0.07495987,  0.06413227,  0.14441176,\n",
       "        0.60894334,  0.17463093,  0.05335403, -0.01273871,  0.03474107,\n",
       "       -0.8123879 , -0.04688699,  0.20193407,  0.2031118 , -0.03935686,\n",
       "        0.06967544, -0.01553638, -0.03405238, -0.06528071,  0.12250231,\n",
       "        0.13991883, -0.17446303, -0.08011883,  0.0849521 , -0.01041659,\n",
       "       -0.13705009,  0.20127155,  0.10069408,  0.00653003,  0.01685157],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_unk = np.mean(glove_embedding_model.vectors, axis=0)\n",
    "glove_embedding_model.add(UNK_TOKEN, glove_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.849383Z",
     "iopub.status.busy": "2021-01-10T15:32:57.849055Z",
     "iopub.status.idle": "2021-01-10T15:32:57.912141Z",
     "shell.execute_reply": "2021-01-10T15:32:57.911104Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.849342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12920076, -0.28866628, -0.01224866, -0.05676644, -0.20210965,\n",
       "       -0.08389011,  0.33359843,  0.16045167,  0.03867431,  0.17833012,\n",
       "        0.04696583, -0.00285802,  0.29099807,  0.04613704, -0.20923874,\n",
       "       -0.06613114, -0.06822549,  0.07665912,  0.3134014 ,  0.17848536,\n",
       "       -0.1225775 , -0.09916984, -0.07495987,  0.06413227,  0.14441176,\n",
       "        0.60894334,  0.17463093,  0.05335403, -0.01273871,  0.03474107,\n",
       "       -0.8123879 , -0.04688699,  0.20193407,  0.2031118 , -0.03935686,\n",
       "        0.06967544, -0.01553638, -0.03405238, -0.06528071,  0.12250231,\n",
       "        0.13991883, -0.17446303, -0.08011883,  0.0849521 , -0.01041659,\n",
       "       -0.13705009,  0.20127155,  0.10069408,  0.00653003,  0.01685157],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_model[UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(glove_embedding_model.vocab.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.388532Z",
     "iopub.status.busy": "2021-01-10T15:32:57.388190Z",
     "iopub.status.idle": "2021-01-10T15:32:57.513140Z",
     "shell.execute_reply": "2021-01-10T15:32:57.512051Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.388481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(np.all(glove_embedding_model.vectors == 0, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.515317Z",
     "iopub.status.busy": "2021-01-10T15:32:57.514997Z",
     "iopub.status.idle": "2021-01-10T15:32:57.776651Z",
     "shell.execute_reply": "2021-01-10T15:32:57.775349Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.515277Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_embedding_model.add(PAD_TOKEN, np.zeros((1, GLOVE_EMBEDDING_DIMENSION)))\n",
    "glove_embedding_model[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.979844Z",
     "iopub.status.busy": "2021-01-10T15:32:57.979543Z",
     "iopub.status.idle": "2021-01-10T15:32:58.089126Z",
     "shell.execute_reply": "2021-01-10T15:32:58.088010Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.979807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_embedding_model.vocab.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:57.779094Z",
     "iopub.status.busy": "2021-01-10T15:32:57.778585Z",
     "iopub.status.idle": "2021-01-10T15:32:57.847652Z",
     "shell.execute_reply": "2021-01-10T15:32:57.846594Z",
     "shell.execute_reply.started": "2021-01-10T15:32:57.779052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:58.091049Z",
     "iopub.status.busy": "2021-01-10T15:32:58.090730Z",
     "iopub.status.idle": "2021-01-10T15:32:58.354523Z",
     "shell.execute_reply": "2021-01-10T15:32:58.353141Z",
     "shell.execute_reply.started": "2021-01-10T15:32:58.091008Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_vocab = dict(\n",
    "    zip(glove_embedding_model.index2word, range(len(glove_embedding_model.index2word)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:32:58.356831Z",
     "iopub.status.busy": "2021-01-10T15:32:58.356492Z",
     "iopub.status.idle": "2021-01-10T15:32:59.279985Z",
     "shell.execute_reply": "2021-01-10T15:32:59.278846Z",
     "shell.execute_reply.started": "2021-01-10T15:32:58.356790Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_embedding_layer = nn.Embedding(\n",
    "    glove_embedding_model.vectors.shape[0],\n",
    "    GLOVE_EMBEDDING_DIMENSION,\n",
    "    padding_idx=glove_vocab[PAD_TOKEN],\n",
    ")\n",
    "glove_embedding_layer.weight = nn.Parameter(\n",
    "    torch.from_numpy(glove_embedding_model.vectors)\n",
    ")\n",
    "glove_embedding_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:43:56.940860Z",
     "iopub.status.busy": "2021-01-12T10:43:56.940414Z",
     "iopub.status.idle": "2021-01-12T10:43:57.009657Z",
     "shell.execute_reply": "2021-01-12T10:43:57.008458Z",
     "shell.execute_reply.started": "2021-01-12T10:43:56.940814Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_CONTEXT_TOKENS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:43:58.174362Z",
     "iopub.status.busy": "2021-01-12T10:43:58.174034Z",
     "iopub.status.idle": "2021-01-12T10:43:59.662521Z",
     "shell.execute_reply": "2021-01-12T10:43:59.661111Z",
     "shell.execute_reply.started": "2021-01-12T10:43:58.174321Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_question_tokenizer = Tokenizer(WordLevel(glove_vocab, unk_token=UNK_TOKEN))\n",
    "baseline_question_tokenizer.normalizer = Sequence([StripAccents(), Lowercase(), Strip()])\n",
    "baseline_question_tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "baseline_question_tokenizer.enable_padding(\n",
    "    direction=\"right\",\n",
    "    pad_id=glove_vocab[PAD_TOKEN],\n",
    "    pad_type_id=1,\n",
    "    pad_token=PAD_TOKEN\n",
    ")\n",
    "\n",
    "baseline_context_tokenizer = Tokenizer(WordLevel(glove_vocab, unk_token=UNK_TOKEN))\n",
    "baseline_context_tokenizer.normalizer = Sequence([StripAccents(), Lowercase(), Strip()])\n",
    "baseline_context_tokenizer.pre_tokenizer = PreSequence([Whitespace(), Punctuation()])\n",
    "baseline_context_tokenizer.enable_padding(\n",
    "    direction=\"right\",\n",
    "    pad_id=glove_vocab[PAD_TOKEN],\n",
    "    pad_type_id=1,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    length=MAX_CONTEXT_TOKENS,\n",
    ")\n",
    "baseline_context_tokenizer.enable_truncation(MAX_CONTEXT_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:30:36.171511Z",
     "iopub.status.busy": "2021-01-12T10:30:36.171047Z",
     "iopub.status.idle": "2021-01-12T10:31:19.682900Z",
     "shell.execute_reply": "2021-01-12T10:31:19.681226Z",
     "shell.execute_reply.started": "2021-01-12T10:30:36.171456Z"
    }
   },
   "outputs": [],
   "source": [
    "def lost_answers_indexes(df, tokenized_contexts):\n",
    "    whole_answers_start = df[\"answer_start\"].tolist()\n",
    "    whole_answers_end = df[\"answer_end\"].tolist()\n",
    "    lost_dirty, lost_truncated = [], []\n",
    "    for i, (c, s, e) in enumerate(\n",
    "        zip(tokenized_contexts, whole_answers_start, whole_answers_end)\n",
    "    ):\n",
    "        offsets = torch.tensor(c.offsets)[torch.tensor(c.attention_mask).bool()]\n",
    "        start_index = torch.nonzero(offsets[:, 0] == s)\n",
    "        end_index = torch.nonzero(offsets[:, 1] == e)\n",
    "        if len(start_index) == 0 or len(end_index) == 0:\n",
    "            if s > offsets[-1, 0] or e > offsets[-1, 1]:\n",
    "                lost_truncated.append(i)\n",
    "            else:\n",
    "                lost_dirty.append(i)\n",
    "\n",
    "    return lost_truncated, lost_dirty\n",
    "\n",
    "\n",
    "tokenized_contexts = baseline_context_tokenizer.encode_batch(\n",
    "    squad_dataset.dataframe[\"context\"].tolist()\n",
    ")\n",
    "lost_truncated, lost_dirty = lost_answers_indexes(\n",
    "    squad_dataset.dataframe, tokenized_contexts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:31:27.383216Z",
     "iopub.status.busy": "2021-01-12T10:31:27.382783Z",
     "iopub.status.idle": "2021-01-12T10:31:27.454030Z",
     "shell.execute_reply": "2021-01-12T10:31:27.452992Z",
     "shell.execute_reply.started": "2021-01-12T10:31:27.383171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost 183/87599 (0.21%) answers, because of truncation\n",
      "Lost 260/87599 (0.30%) answers, because of dirtyness\n"
     ]
    }
   ],
   "source": [
    "lost_truncated_perc = len(lost_truncated) / len(squad_dataset.dataframe)\n",
    "lost_dirty_perc = len(lost_dirty) / len(squad_dataset.dataframe)\n",
    "print(\n",
    "    f\"Lost {len(lost_truncated)}/{len(squad_dataset.dataframe)} ({lost_truncated_perc:.2%}) answers, because of truncation\"\n",
    ")\n",
    "print(\n",
    "    f\"Lost {len(lost_dirty)}/{len(squad_dataset.dataframe)} ({lost_dirty_perc:.2%}) answers, because of dirtyness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:31:39.382363Z",
     "iopub.status.busy": "2021-01-12T10:31:39.381902Z",
     "iopub.status.idle": "2021-01-12T10:31:39.460629Z",
     "shell.execute_reply": "2021-01-12T10:31:39.459389Z",
     "shell.execute_reply.started": "2021-01-12T10:31:39.382318Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_df_row(df, index):\n",
    "    row = df.iloc[index]\n",
    "    display(HTML(pd.DataFrame([row]).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:32:21.743503Z",
     "iopub.status.busy": "2021-01-12T10:32:21.743119Z",
     "iopub.status.idle": "2021-01-12T10:32:21.821764Z",
     "shell.execute_reply": "2021-01-12T10:32:21.820324Z",
     "shell.execute_reply.started": "2021-01-12T10:32:21.743460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73891</th>\n",
       "      <td>572fdc43947a6a140053cd76</td>\n",
       "      <td>What country is spoken of in the inscriptions of a country in Northern Africa from 1390-1352 BC ?</td>\n",
       "      <td>Greeks</td>\n",
       "      <td>16034</td>\n",
       "      <td>In Homer's Iliad, the names Danaans (or Danaoi: Δαναοί) and Argives (Argives: Αργείοι) are used to designate the Greek forces opposed to the Trojans. The myth of Danaus, whose origin is Egypt, is a foundation legend of Argos. His daughters Danaides, were forced in Tartarus to carry a jug to fill a bathtub without a bottom. This myth is connected with a task that can never be fulfilled (Sisyphos) and the name can be derived from the PIE root *danu: \"river\". There is not any satisfactory theory on their origin. Some scholars connect Danaans with the Denyen, one of the groups of the sea peoples who attacked Egypt during the reign of Ramesses III (1187-1156 BCE). The same inscription mentions the Weshesh who might have been the Achaeans. The Denyen seem to have been inhabitants of the city Adana in Cilicia. Pottery similar to that of Mycenae itself has been found in Tarsus of Cilicia and it seems that some refugees from the Aegean went there after the collapse of the Mycenean civilization. These Cilicians seem to have been called Dananiyim, the same word as Danaoi who attacked Egypt in 1191 BC along with the Quaouash (or Weshesh) who may be Achaeans. They were also called Danuna according to a Hittite inscription and the same name is mentioned in the Amarna letters. Julius Pokorny reconstructs the name from the PIE root da:-: \"flow, river\", da:-nu: \"any moving liquid, drops\", da: navo \"people living by the river, Skyth. nomadic people (in Rigveda water-demons, fem.Da:nu primordial goddess), in Greek Danaoi, Egypt. Danuna\". It is also possible that the name Danaans is pre-Greek. A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt from Amenophis III (1390-1352 BC), Thutmosis III (1437 BC).</td>\n",
       "      <td>A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt</td>\n",
       "      <td>1601</td>\n",
       "      <td>1696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_trunc_index = random.choice(lost_truncated)\n",
    "show_df_row(squad_dataset.dataframe, random_trunc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:32:45.355280Z",
     "iopub.status.busy": "2021-01-12T10:32:45.354830Z",
     "iopub.status.idle": "2021-01-12T10:32:45.433537Z",
     "shell.execute_reply": "2021-01-12T10:32:45.431896Z",
     "shell.execute_reply.started": "2021-01-12T10:32:45.355235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58329</th>\n",
       "      <td>5727e7654b864d1900163faf</td>\n",
       "      <td>What were the nuns allowed to minimally do?</td>\n",
       "      <td>Dominican_Order</td>\n",
       "      <td>12565</td>\n",
       "      <td>Women could not be professed to the Dominican religious life before the age of thirteen. The formula for profession contained in the Constitutions of Montargis Priory (1250) demands that nuns pledge obedience to God, the Blessed Virgin, their prioress and her successors according to the Rule of St. Augustine and the institute of the order, until death. The clothing of the sisters consisted of a white tunic and scapular, a leather belt, a black mantle, and a black veil. Candidates to profession were tested to reveal whether they were actually married women who had merely separated from their husbands. Their intellectual abilities were also tested. Nuns were to be silent in places of prayer, the cloister, the dormitory, and refectory. Silence was maintained unless the prioress granted an exception for a specific cause. Speaking was allowed in the common parlor, but it was subordinate to strict rules, and the prioress, subprioress or other senior nun had to be present.</td>\n",
       "      <td>Speak</td>\n",
       "      <td>829</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_dirty_index = random.choice(lost_dirty)\n",
    "show_df_row(squad_dataset.dataframe, random_dirty_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:40:39.815452Z",
     "iopub.status.busy": "2021-01-12T10:40:39.815005Z",
     "iopub.status.idle": "2021-01-12T10:40:39.912507Z",
     "shell.execute_reply": "2021-01-12T10:40:39.911142Z",
     "shell.execute_reply.started": "2021-01-12T10:40:39.815407Z"
    }
   },
   "outputs": [],
   "source": [
    "to_remove = lost_truncated + lost_dirty\n",
    "df = squad_dataset.dataframe.drop(to_remove)\n",
    "assert len(df) == len(squad_dataset.dataframe) - len(to_remove), (\n",
    "    f\"Before {len(squad_dataset.dataframe)},\n",
    "    f\"After {len(df)}\",\n",
    "    f\"Removed {len(to_remove)}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-12T10:42:02.206522Z",
     "iopub.status.busy": "2021-01-12T10:42:02.206055Z",
     "iopub.status.idle": "2021-01-12T10:42:02.386904Z",
     "shell.execute_reply": "2021-01-12T10:42:02.385551Z",
     "shell.execute_reply.started": "2021-01-12T10:42:02.206477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>188</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>279</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>381</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>92</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>229</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>414</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>476</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>1975</td>\n",
       "      <td>199</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>18890</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87156 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          index  \\\n",
       "0      5733be284776f41900661182   \n",
       "1      5733be284776f4190066117f   \n",
       "2      5733be284776f41900661180   \n",
       "3      5733be284776f41900661181   \n",
       "4      5733be284776f4190066117e   \n",
       "...                         ...   \n",
       "87594  5735d259012e2f140011a09d   \n",
       "87595  5735d259012e2f140011a09e   \n",
       "87596  5735d259012e2f140011a09f   \n",
       "87597  5735d259012e2f140011a0a0   \n",
       "87598  5735d259012e2f140011a0a1   \n",
       "\n",
       "                                                question  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595               What was Yangon previously known as?   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598                      What is KMC an initialism of?   \n",
       "\n",
       "                          title  context_id  \\\n",
       "0      University_of_Notre_Dame           0   \n",
       "1      University_of_Notre_Dame           0   \n",
       "2      University_of_Notre_Dame           0   \n",
       "3      University_of_Notre_Dame           0   \n",
       "4      University_of_Notre_Dame           0   \n",
       "...                         ...         ...   \n",
       "87594                 Kathmandu       18890   \n",
       "87595                 Kathmandu       18890   \n",
       "87596                 Kathmandu       18890   \n",
       "87597                 Kathmandu       18890   \n",
       "87598                 Kathmandu       18890   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87598  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                        answer  answer_start  answer_end  \n",
       "0                   Saint Bernadette Soubirous           515         541  \n",
       "1                    a copper statue of Christ           188         213  \n",
       "2                            the Main Building           279         296  \n",
       "3      a Marian place of prayer and reflection           381         420  \n",
       "4           a golden statue of the Virgin Mary            92         126  \n",
       "...                                        ...           ...         ...  \n",
       "87594                                   Oregon           229         235  \n",
       "87595                                  Rangoon           414         421  \n",
       "87596                                    Minsk           476         481  \n",
       "87597                                     1975           199         203  \n",
       "87598              Kathmandu Metropolitan City             0          27  \n",
       "\n",
       "[87156 rows x 8 columns]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.update_df(df)\n",
    "squad_dataset.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T15:33:18.219925Z",
     "iopub.status.busy": "2021-01-10T15:33:18.219546Z",
     "iopub.status.idle": "2021-01-10T15:33:18.310899Z",
     "shell.execute_reply": "2021-01-10T15:33:18.309469Z",
     "shell.execute_reply.started": "2021-01-10T15:33:18.219881Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaselineDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, question_tokenizer, context_tokenizer):\n",
    "        self.question_tokenizer = question_tokenizer\n",
    "        self.context_tokenizer = context_tokenizer\n",
    "\n",
    "    def find_tokenized_answer_indexes(self, offsets, query, dim):\n",
    "        index = torch.nonzero(offsets[:, dim] == query)\n",
    "        assert len(index) in (0, 1)\n",
    "        return index.item() if len(index) > 0 else -1\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        (questions, contexts, answers_start, answers_end) = zip(*inputs)\n",
    "        tokenized_questions = self.question_tokenizer.encode_batch(questions)\n",
    "        tokenized_contexts = self.context_tokenizer.encode_batch(contexts)\n",
    "        batch_size = len(tokenized_questions)\n",
    "        questions_shape = (batch_size, len(tokenized_questions[0].ids))\n",
    "        contexts_shape = (batch_size, len(tokenized_contexts[0].ids))\n",
    "\n",
    "        batch = {\n",
    "            \"question_ids\": torch.empty(questions_shape, dtype=torch.long),\n",
    "            \"question_type_ids\": torch.empty(questions_shape, dtype=torch.long),\n",
    "            \"question_attention_mask\": torch.empty(questions_shape, dtype=torch.bool),\n",
    "            \"question_special_tokens_mask\": torch.empty(\n",
    "                questions_shape, dtype=torch.bool\n",
    "            ),\n",
    "            \"question_lenghts\": torch.empty((batch_size,), dtype=torch.long),\n",
    "            \"context_ids\": torch.empty(contexts_shape, dtype=torch.long),\n",
    "            \"context_type_ids\": torch.empty(contexts_shape, dtype=torch.long),\n",
    "            \"context_attention_mask\": torch.empty(contexts_shape, dtype=torch.bool),\n",
    "            \"context_special_tokens_mask\": torch.empty(\n",
    "                contexts_shape, dtype=torch.bool\n",
    "            ),\n",
    "            \"context_offsets\": torch.empty((*contexts_shape, 2), dtype=torch.long),\n",
    "            \"context_lenghts\": torch.empty((batch_size,), dtype=torch.long),\n",
    "            # \"answer_start\": torch.tensor(answers_start, dtype=torch.long),\n",
    "            # \"answer_end\": torch.tensor(answers_end, dtype=torch.long),\n",
    "            \"answer_start\": torch.empty((batch_size,), dtype=torch.long),\n",
    "            \"answer_end\": torch.empty((batch_size,), dtype=torch.long),\n",
    "        }\n",
    "        for i in range(batch_size):\n",
    "            batch[\"question_ids\"][i] = torch.tensor(tokenized_questions[i].ids)\n",
    "            batch[\"question_type_ids\"][i] = torch.tensor(\n",
    "                tokenized_questions[i].type_ids\n",
    "            )\n",
    "            batch[\"question_attention_mask\"][i] = torch.tensor(\n",
    "                tokenized_questions[i].attention_mask\n",
    "            )\n",
    "            batch[\"question_special_tokens_mask\"][i] = torch.tensor(\n",
    "                tokenized_questions[i].special_tokens_mask\n",
    "            )\n",
    "            batch[\"question_lenghts\"][i] = torch.count_nonzero(\n",
    "                ~batch[\"question_special_tokens_mask\"][i]\n",
    "            )\n",
    "            batch[\"context_ids\"][i] = torch.tensor(tokenized_contexts[i].ids)\n",
    "            batch[\"context_type_ids\"][i] = torch.tensor(tokenized_contexts[i].type_ids)\n",
    "            batch[\"context_attention_mask\"][i] = torch.tensor(\n",
    "                tokenized_contexts[i].attention_mask\n",
    "            )\n",
    "            batch[\"context_special_tokens_mask\"][i] = torch.tensor(\n",
    "                tokenized_contexts[i].special_tokens_mask\n",
    "            )\n",
    "            batch[\"context_offsets\"][i] = torch.tensor(tokenized_contexts[i].offsets)\n",
    "            batch[\"context_lenghts\"][i] = torch.count_nonzero(\n",
    "                ~batch[\"context_special_tokens_mask\"][i]\n",
    "            )\n",
    "            masked_offsets = batch[\"context_offsets\"][i][batch[\"context_attention_mask\"][i]]\n",
    "            batch[\"answer_start\"][i] = self.find_tokenized_answer_indexes(\n",
    "                masked_offsets, answers_start[i], 0\n",
    "            )\n",
    "            batch[\"answer_end\"][i] = self.find_tokenized_answer_indexes(\n",
    "                masked_offsets, answers_end[i], 1\n",
    "            )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:25:45.416792Z",
     "iopub.status.busy": "2021-01-10T16:25:45.416352Z",
     "iopub.status.idle": "2021-01-10T16:25:45.496753Z",
     "shell.execute_reply": "2021-01-10T16:25:45.495651Z",
     "shell.execute_reply.started": "2021-01-10T16:25:45.416747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101400"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = model.QABaselineModel(glove_embedding_layer, MAX_CONTEXT_TOKENS)\n",
    "baseline_model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:20:06.000943Z",
     "iopub.status.busy": "2021-01-10T16:20:06.000477Z",
     "iopub.status.idle": "2021-01-10T16:20:06.071891Z",
     "shell.execute_reply": "2021-01-10T16:20:06.070712Z",
     "shell.execute_reply.started": "2021-01-10T16:20:06.000889Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    logging_dir=\"./runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=10,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=64,\n",
    "    label_names=[\"answer_start\", \"answer_end\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:20:06.850442Z",
     "iopub.status.busy": "2021-01-10T16:20:06.850103Z",
     "iopub.status.idle": "2021-01-10T16:20:06.934366Z",
     "shell.execute_reply": "2021-01-10T16:20:06.932424Z",
     "shell.execute_reply.started": "2021-01-10T16:20:06.850401Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'ExponentialLR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-06103baa3cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquad_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquad_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#callbacks=[transformers.integrations.WandbCallback]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'ExponentialLR'"
     ]
    }
   ],
   "source": [
    "baseline_trainer = transformers.Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_args,\n",
    "    data_collator=BaselineDataCollatorWithPadding(baseline_question_tokenizer, baseline_context_tokenizer),\n",
    "    train_dataset=squad_dataset.train_dataset,\n",
    "    eval_dataset=squad_dataset.val_dataset,\n",
    "    #callbacks=[transformers.integrations.WandbCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:15:51.844979Z",
     "iopub.status.busy": "2021-01-10T16:15:51.844657Z",
     "iopub.status.idle": "2021-01-10T16:16:57.231529Z",
     "shell.execute_reply": "2021-01-10T16:16:57.229401Z",
     "shell.execute_reply.started": "2021-01-10T16:15:51.844937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 300, 1]) torch.Size([256, 300, 1])\n",
      "tensor(18.8223, grad_fn=<NllLossBackward>) tensor(16.5142, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-a28822d33159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiDAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:45:21.848643Z",
     "iopub.status.busy": "2021-01-10T16:45:21.848207Z",
     "iopub.status.idle": "2021-01-10T16:45:21.934780Z",
     "shell.execute_reply": "2021-01-10T16:45:21.933493Z",
     "shell.execute_reply.started": "2021-01-10T16:45:21.848598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314350"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidaf_model = model.BiDAFModel(glove_embedding_layer)\n",
    "bidaf_model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:45:22.629608Z",
     "iopub.status.busy": "2021-01-10T16:45:22.629295Z",
     "iopub.status.idle": "2021-01-10T16:45:22.699341Z",
     "shell.execute_reply": "2021-01-10T16:45:22.697571Z",
     "shell.execute_reply.started": "2021-01-10T16:45:22.629567Z"
    }
   },
   "outputs": [],
   "source": [
    "bidaf_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    logging_dir=\"./runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=64,\n",
    "    label_names=[\"answer_start\", \"answer_end\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:45:22.851387Z",
     "iopub.status.busy": "2021-01-10T16:45:22.851019Z",
     "iopub.status.idle": "2021-01-10T16:45:22.918908Z",
     "shell.execute_reply": "2021-01-10T16:45:22.917726Z",
     "shell.execute_reply.started": "2021-01-10T16:45:22.851346Z"
    }
   },
   "outputs": [],
   "source": [
    "bidaf_optimizer = optim.Adadelta(bidaf_model.parameters(), lr=0.5)\n",
    "bidaf_lr_scheduler = optim.lr_scheduler.ExponentialLR(bidaf_optimizer, gamma=.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:45:24.289358Z",
     "iopub.status.busy": "2021-01-10T16:45:24.289042Z",
     "iopub.status.idle": "2021-01-10T16:45:24.363955Z",
     "shell.execute_reply": "2021-01-10T16:45:24.362912Z",
     "shell.execute_reply.started": "2021-01-10T16:45:24.289318Z"
    }
   },
   "outputs": [],
   "source": [
    "bidaf_trainer = transformers.Trainer(\n",
    "    model=bidaf_model,\n",
    "    args=bidaf_args,\n",
    "    data_collator=BaselineDataCollatorWithPadding(baseline_question_tokenizer, baseline_context_tokenizer),\n",
    "    train_dataset=squad_dataset.train_dataset,\n",
    "    eval_dataset=squad_dataset.val_dataset,\n",
    "    optimizers=(bidaf_optimizer, bidaf_lr_scheduler),\n",
    "    #callbacks=[transformers.integrations.WandbCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-10T16:45:33.062428Z",
     "iopub.status.busy": "2021-01-10T16:45:33.061976Z",
     "iopub.status.idle": "2021-01-10T16:46:34.743864Z",
     "shell.execute_reply": "2021-01-10T16:46:34.741691Z",
     "shell.execute_reply.started": "2021-01-10T16:45:33.062384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 300, 1]) torch.Size([256, 300, 1])\n",
      "tensor(15.0205, grad_fn=<NllLossBackward>) tensor(13.8060, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-a28822d33159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T16:35:27.908837Z",
     "iopub.status.busy": "2021-01-05T16:35:27.908413Z",
     "iopub.status.idle": "2021-01-05T16:35:37.556426Z",
     "shell.execute_reply": "2021-01-05T16:35:37.555013Z",
     "shell.execute_reply.started": "2021-01-05T16:35:27.908792Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T15:11:43.846188Z",
     "iopub.status.busy": "2021-01-09T15:11:43.845741Z",
     "iopub.status.idle": "2021-01-09T15:11:43.917117Z",
     "shell.execute_reply": "2021-01-09T15:11:43.915835Z",
     "shell.execute_reply.started": "2021-01-09T15:11:43.846141Z"
    }
   },
   "outputs": [],
   "source": [
    "args = transformers.TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    logging_dir=\"./runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    label_names=[\"answer_start\", \"answer_end\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T15:11:43.919760Z",
     "iopub.status.busy": "2021-01-09T15:11:43.919407Z",
     "iopub.status.idle": "2021-01-09T15:11:43.983226Z",
     "shell.execute_reply": "2021-01-09T15:11:43.982049Z",
     "shell.execute_reply.started": "2021-01-09T15:11:43.919720Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, padding=True, max_lenght=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = True\n",
    "        self.max_length = None\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        (questions, contexts, _, _) = zip(*inputs)\n",
    "        tokenized = self.tokenizer(questions, contexts)\n",
    "        batch = self.tokenizer.pad(\n",
    "            tokenized,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T15:11:45.132472Z",
     "iopub.status.busy": "2021-01-09T15:11:45.132147Z",
     "iopub.status.idle": "2021-01-09T15:11:45.212777Z",
     "shell.execute_reply": "2021-01-09T15:11:45.210886Z",
     "shell.execute_reply.started": "2021-01-09T15:11:45.132429Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-a9a753d614d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-01-09T15:11:45.214014Z",
     "iopub.status.idle": "2021-01-09T15:11:45.214520Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=squad_dataset.train_dataset,\n",
    "    eval_dataset=squad_dataset.val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T16:37:10.984314Z",
     "iopub.status.busy": "2021-01-05T16:37:10.983997Z",
     "iopub.status.idle": "2021-01-05T16:37:23.456420Z",
     "shell.execute_reply": "2021-01-05T16:37:23.454650Z",
     "shell.execute_reply.started": "2021-01-05T16:37:10.984273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;31m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_local_process_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
